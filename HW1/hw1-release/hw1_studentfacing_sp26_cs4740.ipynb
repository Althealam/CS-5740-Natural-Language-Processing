{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ce9f8d",
   "metadata": {
    "id": "d6ce9f8d"
   },
   "source": [
    "# Homework 1: Logistic Regression Classificaton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76d7c3",
   "metadata": {
    "id": "9c76d7c3"
   },
   "source": [
    "In this assignment, you will train a binary text classification model, specifically a logistic regression model. The binary text classification task we will work with is \"Text Entailment\", a classical NLP task.\n",
    "You will write the code to process the data, extract features, and train a logistic regression model. You will also conduct some feature engineering of your own to try to improve upon the naive approach.\n",
    "\n",
    "\n",
    "__Deadlines__: This assignment is due on <font color=\"red\">(Feb 21, 11.59 p.m.)</font>. This notebook will walk you through the assignment step-by-step, including how to make the final submission.\n",
    "\n",
    "__Tl;dr of structure of the assignment__: In Section 1 and Section 2, you will learn to manipulate the input data and extract simple n-gram features. In Section 3, you will implement the logistic regression training algorithm. In Section 4, you will conduct your own additional feature engineering to improve the performance of the logistic regression model.\n",
    "\n",
    "\n",
    "__Policies.__ All the policies described on the course website are applicable as is (including the policy on academic integrity and the use of generative AI tools). For more information, see: https://www.cs.cornell.edu/courses/cs4740/2026sp.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518e0cf9",
   "metadata": {
    "id": "518e0cf9"
   },
   "source": [
    "### Notes:\n",
    "  \n",
    "- You will **NOT** be submitting this .ipynb file. Please refer to the submission instructions in both the hw1 pdf shared with you and at the end of this notebook.\n",
    "- This notebook includes some written questions, such as creating graphs, computing data statistics, etc. You will include the answers to these written questions in the same pdf document where you attempt Section A of the homework.\n",
    "- Do **NOT** add, remove, or modify any imports across python source files. If you have any concerns regarding missing imports, please let course staff know through Ed before attempting to change anything.\n",
    "- Do **NOT** change any of the function headers and/or specs! The input(s) and output must perfectly match the specs, or else your implementation for any function with changed specs will most likely fail!\n",
    "- If you decide to create local helper functions, your code must have docstrings/comments documenting the meaning of parameters and important parameter-like variables.\n",
    "- We are recommending python version 3.9+. This is due to compatibility issues with the external dependencies.\n",
    "\n",
    "\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0226fa",
   "metadata": {
    "id": "7c0226fa"
   },
   "source": [
    "## Part 0: Imports and Installs\n",
    "\n",
    "Run the following code blocks to connect to the right google drive folder and install any external libraries and needed packages to run HW1 assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pdfaGVjvAEKW",
   "metadata": {
    "id": "pdfaGVjvAEKW"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6FOMjxJnAOLw",
   "metadata": {
    "id": "6FOMjxJnAOLw"
   },
   "outputs": [],
   "source": [
    "# set to location where you uploaded directory\n",
    "%cd \"/content/drive/MyDrive/CS4740-sp26/hw1-release/\"\n",
    "#%pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "import sys\n",
    "from importlib import reload\n",
    "\n",
    "# Create a fake 'imp' module with just the reload function\n",
    "class ImpModule:\n",
    "    reload = staticmethod(reload)\n",
    "\n",
    "sys.modules['imp'] = ImpModule()\n",
    "\n",
    "\n",
    "import IPython\n",
    "\n",
    "ipython = IPython.get_ipython()\n",
    "ipython.run_line_magic(\"sx\", f\"chmod +x scripts/*.py\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623f20d",
   "metadata": {
    "id": "7623f20d"
   },
   "outputs": [],
   "source": [
    "### IMPORTS -- DO NOT MODIFY ###\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from data_exploration import unzip_data, read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d37d4c",
   "metadata": {
    "id": "d6d37d4c"
   },
   "source": [
    "## Part 1: Data Exploration\n",
    "\n",
    "**Natural Language Inference** or **Recognizing Textual Entailment** is a classifical NLP task, the goal of which is to determine the inference relationship between two pairs of words (see \"Natural logic and natural language inference\" by MacCartney and Manning 2008 to read more).\n",
    "\n",
    "The goal is to determine whether a natural langauge hypothesis (H) can be reasonably inferred from a give premise (P). The original task classifies each (P, H) pair into one of three classes: ***{entailed, contradicted, neutral}***.\n",
    "\n",
    "Consider the following example:\n",
    "- P: \"Children are smiling and waving at camera\"\n",
    "- H1: \"The kids are frowning\"\n",
    "- H2: \"There are children present\"\n",
    "- H3: \"The kids are eating\"\n",
    "\n",
    "In this example, the premise P contradicts the hypothesis H1. Hence, (P, H1) has the label ***contradiction***. The premise P supports or entails H2, hence (P, H2) has the label ***entailment***. Finally, the hypothesis H3 is neither supported nor directly contradicted by the premise, and therefore has the label ***neutral***.\n",
    "\n",
    "\n",
    "### Our task in HW1: Binary Text Classification\n",
    "In this assignment, we will only consider the <font color=\"red\">**binary text classification task**</font> , and therefore our label set will be ***{0: contradiction, 1: entailment}***. We will work with the [Stanford Natural Language Inference (SNLI) dataset](https://nlp.stanford.edu/projects/snli/). This data is stored in a zip file. You can use the following provided function to load the data and preprocess it. Under the hood, this unzips the data and reads each of the provided json data files into Python dictionaries. It then further formats the data such that our downstream code can ingest it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797de107",
   "metadata": {
    "id": "797de107"
   },
   "outputs": [],
   "source": [
    "data_zip_path = \"dataset.zip\"\n",
    "dest_path = \"dataset\"\n",
    "\n",
    "unzip_data(data_zip_path, dest_path) # unzips the data into current directory\n",
    "training_data = read_csv(os.path.join(dest_path, \"train.csv\"))\n",
    "validation_data = read_csv(os.path.join(dest_path, \"validation.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498105f9",
   "metadata": {
    "id": "498105f9"
   },
   "source": [
    "### Looking at the data\n",
    "Since your data files can be large and unwieldy, you can explore the data by writing code. Check out the data format by looking at at keys, and some of the values in the data. You can use the following code to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb853ba",
   "metadata": {
    "id": "cbb853ba"
   },
   "outputs": [],
   "source": [
    "print(training_data.keys())\n",
    "print(validation_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f384856",
   "metadata": {
    "id": "3f384856"
   },
   "source": [
    "To get a sense of what your data looks like, look at some samples. Run the following code block multiple times to see at many random samples. Note that our dataset only contains binary labels where `0` indicates that the premise contradicts the hypothesis (i.e. not entailment), and `1` means that the premise entails the hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06b5a2",
   "metadata": {
    "id": "4f06b5a2"
   },
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(training_data['premise']))\n",
    "premise = training_data['premise'][random_index]\n",
    "hyp = training_data['hypothesis'][random_index]\n",
    "label = training_data['label'][random_index]\n",
    "print(f\"premise: {premise}\")\n",
    "print(f\"hypothesis: {hyp}\")\n",
    "print(f\"label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I_T2KRNrp7gJ",
   "metadata": {
    "id": "I_T2KRNrp7gJ"
   },
   "source": [
    "We have already included code in `data.py` to tokenize both the premise and hypothesis. We will store each datapoint as an object of the class `Example` defined in `data.py`. Each `Example` object stores a list of tokens from the premise, list of tokens from the hypothesis, and label for the data point. Run the following code block to create this list of Example objects for both the training and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773067c",
   "metadata": {
    "id": "9773067c"
   },
   "outputs": [],
   "source": [
    "from data import read_examples\n",
    "\n",
    "print(f\"\\n Cleaning up and tokenizing train data...\")\n",
    "train_exs = read_examples(training_data)\n",
    "val_exs = read_examples(validation_data)\n",
    "\n",
    "\n",
    "print(train_exs[random_index])\n",
    "print(len(train_exs))\n",
    "print(len(val_exs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b11725a",
   "metadata": {
    "id": "0b11725a"
   },
   "source": [
    "### Data Statistics\n",
    "To get a better feel for the data, let's visualize some of its characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4f865",
   "metadata": {
    "id": "fdd4f865"
   },
   "source": [
    "#### <font color=\"brown\"> Q1.1 Plot histograms of the number of tokens in the premise and hypothesis inputs separately. Also, generate the same plots for datapoints that have the **entailment** label and the **contradiction** separately. Do you notice anything interesting about this data? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df234516",
   "metadata": {
    "id": "df234516"
   },
   "source": [
    "#### <font color=\"brown\"> Q1.2: Find the five most frequent unigrams (single tokens) in the training data, and list them out. What are the five most frequent bigrams (two token subsequences)? What do you notice? </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dd8260",
   "metadata": {
    "id": "f0dd8260"
   },
   "source": [
    "As an example, here are the most frequent unigrams and bigrams for the following toy dataset of three sentences.\n",
    "\n",
    "* [\"i\", \"love\", \"my\", \"cat\"]\n",
    "* [\"i\", \"love\", \"dogs\"]\n",
    "* [\"my\", \"cat\", \"is\", \"cute\"]\n",
    "\n",
    "Unigram counts:\n",
    "* \"i\": 2\n",
    "* \"cat\": 2\n",
    "* \"love\": 2\n",
    "* \"my\": 2\n",
    "* \"dogs\": 1\n",
    "* \"is\": 1\n",
    "* \"cute\": 1\n",
    "\n",
    "Bigram counts:\n",
    "* \"i love\": 2\n",
    "* \"my cat\": 2\n",
    "* all other bigrams have frequency one (\"love my\", \"love dogs\", \"cat is\", \"is cute\")\n",
    "\n",
    "4-gram counts:\n",
    "* \"i love my cat\": 1\n",
    "* \"my cat is cute\": 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381aa9fa",
   "metadata": {
    "id": "381aa9fa"
   },
   "source": [
    "## Part 2: Feature Extraction\n",
    "In order to train our logistic regression model, we need to turn our natural language data into a feature vector. In this assignment, we will explore various strategies for constructing our feature vectors, but start with a simple n-gram based approach.\n",
    "\n",
    "\n",
    "The first step for feature engineering is preprocessing a text input into tokenization. We have already implemented this for you in Part 1. In this section, you will be responsible for implementing feature extractors that convert a tokenized list of words into a sparse feature representation with Python dictionaries and lists.\n",
    "\n",
    "A common and effective way to build feature vectors for models such as logistic regression is to use n-gram features. An n-gram is a contiguous sequence of n tokens from a sentence.\n",
    "\n",
    "* Unigrams are single words (e.g., \"dog\", \"running\")\n",
    "* Bigrams are pairs of consecutive words (e.g., \"the dog\", \"is running\")\n",
    "\n",
    "**Important**: Remember that our `Example` object has two input texts, stored as a list of tokens, namely the premise and the hypothesis. We need to provide both these as input to the logistic regression model. We define a function `get_combined_words` to combine these into a single list. The function uses the `[SEP]` token to distinguish the two. Remember to perform all feature engineering on this combined list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d64b09",
   "metadata": {
    "id": "b6d64b09"
   },
   "source": [
    "### Part 2.1 Unigram and Bigram Feature Extractors\n",
    "> <font color=\"orange\">File to be modified: `features.py`. \n",
    "\n",
    "\n",
    "In this part, you will complete the ``UnigramFeatureExtractor`` and the ``BigramFeatureExtractor`` classes in `features.py`. For both you will be implementing the ``build_vocabulary(examples)`` and ``extract_features(sentence)``.\n",
    "\n",
    "The complete pipeline for feature extraction during training is the following:\n",
    "\n",
    "a. Initialize an object of the Unigram/Bigram FeatureExtractor class. This will specify the maximum number of features to store via `max_num_features` attribute.\n",
    "\n",
    "b. For the given training data, construct the vocabulary or the feature set. For both the unigram and bigram feature extractors, you must keep only the max_num_features most frequent words in the vocabulary and store these in the ``self.vocabulary`` attribute. You will implement this in ``build_vocabulary(examples)``.\n",
    "\n",
    "c. Once you have created the vocabulary, at training or test time, you will need to create the feature vector for any given input text.  You will implement this logic in ``extract_features(sentence)``. It will take as input a tokenized text, and return a feature dictionary. When constructing this dictionary, Be sure that all keys of this dictionary are present in your vocabulary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17eb350b",
   "metadata": {
    "id": "17eb350b"
   },
   "source": [
    "#### Part 2.2 Let's create the feature dictionaries!\n",
    "Now let's run your unigram and bigram feature extractors on our training dataset. Be sure to manually examine the data ``train_exs`` and your feature vectors to see if everything makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44bd391",
   "metadata": {
    "id": "e44bd391"
   },
   "outputs": [],
   "source": [
    "from features import (\n",
    "    UnigramFeatureExtractor,\n",
    "    BigramFeatureExtractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a365ea",
   "metadata": {
    "id": "75a365ea"
   },
   "outputs": [],
   "source": [
    "unigram_extractor = UnigramFeatureExtractor(max_num_features=2000)\n",
    "bigram_extractor = BigramFeatureExtractor(max_num_features=2000)\n",
    "\n",
    "# build vocab from our training set\n",
    "print(f\"\\nBuilding vocabulary...\")\n",
    "unigram_extractor.build_vocabulary(train_exs)\n",
    "bigram_extractor.build_vocabulary(train_exs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e45d0e",
   "metadata": {
    "id": "76e45d0e"
   },
   "source": [
    "It you want to test your implementation of the above functions, you should write your own small train datasets and see if the constructed vocabulary and feature dictionaries for a given input make sense.\n",
    "\n",
    "Once you are convinced your implementation is correct, run the code below to see the feature dictionaries for the following examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384e7d4",
   "metadata": {
    "id": "3384e7d4"
   },
   "outputs": [],
   "source": [
    "# extract features for this example sentence\n",
    "from data import tokenize_and_clean\n",
    "tokenized = train_exs[1004].get_combined_words()\n",
    "print(' '.join(tokenized))\n",
    "uni_features = unigram_extractor.extract_features(tokenized)\n",
    "print(uni_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87d56cc",
   "metadata": {
    "id": "e87d56cc"
   },
   "outputs": [],
   "source": [
    "# extract features for this example sentence\n",
    "bigram_features = bigram_extractor.extract_features(tokenized)\n",
    "print(bigram_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GOHtM35U4wr7",
   "metadata": {
    "id": "GOHtM35U4wr7"
   },
   "source": [
    " #### <font color=\"brown\"> Q2.1.  Run the unigram and bigram feature extractor on 5 unique examples. Look at the feature vectors they output. Do you expect these to be informative features? Comment on 2 characteristics of these features that you find interesting or surprising. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9341e5",
   "metadata": {
    "id": "4d9341e5"
   },
   "source": [
    "## Part 3: Logistic Regression\n",
    "> <font color=\"orange\">File to be modified: `models.py`.\n",
    "\n",
    "We will be using logistic regression, a linear model for binary classification. It computes a weighted sum of input features and applies a sigmoid function to produce a probability between 0 and 1. Our goal is to learn model parameters (weights and bias) to minimize cross-entropy loss for our data.\n",
    "\n",
    "Let $f(x)$ be the features of an input $x$ ($x$ is the concatenation of premise and hypothesis for our dataset). Let $f_k$ be the $k^{th}$ feature and $w_k$ be its corresponding weight. We define $z = b + \\sum_k w_k f_k$.\n",
    "\n",
    "According to the logistic regression model, $P(y = 1 \\;|\\; f(x) \\,) = \\frac{e^z}{1 + e^z} $. Refer to class notes for more details.\n",
    "\n",
    "We have defined the ``LogisticRegressionClassifier`` class. We initialize the weights $w_k$ and the bias $b$ for our logistic regression classifier to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c23d20",
   "metadata": {
    "id": "d4c23d20"
   },
   "outputs": [],
   "source": [
    "from models import LogisticRegressionClassifier, print_evaluation_metrics\n",
    "\n",
    "\n",
    "## Initialize the classifier with the UnigramFeatureExtractor\n",
    "classifier = LogisticRegressionClassifier(unigram_extractor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XglrpO-XfEPe",
   "metadata": {
    "id": "XglrpO-XfEPe"
   },
   "source": [
    "### Part 3.1: Implement the predict function\n",
    "\n",
    " First, you should implement the ``predict`` function. This function takes in as input the input to the logistic regression model, i.e. list of tokens to be classified. It ouputs the class label (int), i.e. either 0 or 1. Refer to the formula above to compute $P(y=1 | f(x))$. You should return the label (0 or 1) with the higher probability.\n",
    "\n",
    "We suggest using the `sigmoid` function we implemented for you ``models.py``, which takes in $z = b + \\sum_k w_kf_k$ as input. This implementation is numerically stable.\n",
    "\n",
    " #### <font color=\"brown\"> Q3.1.  Remember that our model is untrained and that all weights and the bias were initialized to 0. What should be the predicted label for all datapoints? </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4248ee",
   "metadata": {
    "id": "4f4248ee"
   },
   "source": [
    "### Part 3.2: Implement Binary Cross Entropy Loss\n",
    "In order to learn the weights for the logistic regression model (or any ML model for that matter), we need to have a loss function. The loss function measures how well the model’s predictions match the true labels in the training data. Intuitively, we want to minimize the loss function in order to get a high performing model. You will be taking the gradient with respect to the loss function and using it to update your weight paramters.\n",
    "\n",
    "Let $y$ be the true label for an input (for our case, this will be either 0 or 1) and $\\widehat{y}$ be the probability that the predicted label is 1, i.e. $P(y=1 | x)$.\n",
    "\n",
    "Implement the function ``cross_entropy_loss`` in ``models.py`` using the following binary cross entropy formula:\n",
    "\n",
    "$L(y, \\widehat{y}) = -\\Big[y \\log(\\widehat{y} + \\varepsilon) + (1 - y)\\log(1 - \\widehat{y} + \\varepsilon)\\Big]$\n",
    "\n",
    "#### <font color=\"brown\"> Q3.2: What numerical issue does the small constant $\\epsilon$ prevent in the cross-entropy loss calculation, and why is it important? </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481830e8",
   "metadata": {
    "id": "481830e8"
   },
   "source": [
    "### Part 3.3: Training Loop\n",
    "In this part, you will write the training loop to train logistic regression using **stochastic gradient descent (SGD)**, updating the model after each example.\n",
    "\n",
    "First, let's recap what we've done so far:\n",
    "\n",
    "1. Preprocess our data (reading CSV files, tokenization).\n",
    "2. Extract features (you implemented a Unigram and Bigram feature extractor in Part 2).\n",
    "3. Train a logistic regression classifier (now).\n",
    "\n",
    "The `LogisticRegressionClassifier` stores the weights of the model as a dictionary `weights: dict[str, float]`, where each feature name maps to its weight. The bias is stored as a separate scalar float `bias`.\n",
    "\n",
    "You will implement the function ``train`` for the `LogisticRegressionClassifier`. It takes the entire training data (list of Example) as input, along with the learning rate and number of epochs. The rough pseudocode for the training is:\n",
    "\n",
    "For each epoch:\n",
    "- Shuffle the training data and for each datapoint:  \n",
    "  1. Compute prediction `ŷ = sigmoid(w·x + b)`  \n",
    "  2. Compute loss (cross-entropy)  \n",
    "  3. Compute the gradients of the weights and bias.\n",
    "  4. Update the weight and bias using SGD.\n",
    "\n",
    "__Hint__: You should print metrics like loss, and train accuracy to measure how these change with each training epoch. You should also observe how validation accuracy changes during training. These signals will inform you if your model is training properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671e8a7",
   "metadata": {
    "id": "8671e8a7"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters (TODO play around with these)\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "## train the classifier initialized above\n",
    "classifier.train(train_exs, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeea881",
   "metadata": {
    "id": "beeea881"
   },
   "source": [
    "### Part 3.3: Evaluation\n",
    "Now that you trained your model, let's see how well it does on the validation data. <font color=\"red\"> With the following hyperparameters (max_num_features = 2000, learning_rate = 0.01, num_epochs = 10), you should see a validation accuracy of ~64% or higher using both the unigram and bigram feature extractor. </font>\n",
    "\n",
    "If your accuracy is lower, there is likely an error in one of your function declarations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772dffd7",
   "metadata": {
    "id": "772dffd7"
   },
   "outputs": [],
   "source": [
    "print_evaluation_metrics(classifier, val_exs, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed4a447",
   "metadata": {
    "id": "5ed4a447"
   },
   "source": [
    "#### <font color=\"brown\"> Q3.3: What accuracy metric were you able to get with the unigram and bigram feature extractors? Plot a graph showing the avg train loss per example across different epochs during training. Plot a graph for train and validation accuracies during training. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6RpSWhBxnZY8",
   "metadata": {
    "id": "6RpSWhBxnZY8"
   },
   "source": [
    "#### <font color=\"brown\"> Q3.4: Here you will conduct a hyperparameter search to identify the best hyperparameters for your logisitc regression model. Choose one of the feature extractors (unigram or bigram). Report accuracy results with 5 different learning rates and 5 different max_num_features. You should fix the number of epochs to 10. <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4a9698",
   "metadata": {
    "id": "7d4a9698"
   },
   "source": [
    "## Part 4: FancyFeatureExtraction\n",
    "\n",
    "> <font color=\"orange\">File to be modified: `features.py`.\n",
    "\n",
    "\n",
    "Now it's your turn to do some **feature engineering**. Feature engineering is the process of manually designing \"features\" which should improve model performance based on intuitions that we have about the raw data. For example, if you were trying to classify text sequences as tweets or not, a good feature could be whether or not the input text is more than 280 characters or not (where more than 280 characters is of course a strong negative signal).\n",
    "\n",
    "Be creative and have fun! A good place to start is to continue exploring the raw data and see if there are any trends which distinguish the classes. In this section, you can also consider improvements to our naive tokenization algorithm. Just make sure to limit the number of features that you generate to *number*.\n",
    "\n",
    "Add your fancy feature extraction code to the FancyFeatureExtractor class in ``models.py``. Run the code block below to run training with your fancy feature extractor. Does it give you better performance than the n-gram features?\n",
    "\n",
    "> __Important Note:__ <font color=\"red\"> When we test your FancyFeatureExtractor on our end, we will use the following hyperparameters: max_num_features=2000, learning_rate=0.01, epochs=10</font>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uOdOkpXXl-1k",
   "metadata": {
    "id": "uOdOkpXXl-1k"
   },
   "outputs": [],
   "source": [
    "from features import FancyFeatureExtractor\n",
    "\n",
    "## These are the hyperparameters we will use to test your fancy feature extractor on our end.\n",
    "max_num_features = 2000\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "\n",
    "fancy_feature = FancyFeatureExtractor(max_num_features=max_num_features)\n",
    "fancy_feature.build_vocabulary(train_exs)\n",
    "\n",
    "classifier_fancy = LogisticRegressionClassifier(fancy_feature)\n",
    "classifier_fancy.train(train_exs, learning_rate, epochs)\n",
    "\n",
    "print_evaluation_metrics(classifier_fancy, val_exs, \"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r7SfeImrq5xb",
   "metadata": {
    "id": "r7SfeImrq5xb"
   },
   "source": [
    "#### <font color=\"brown\"> Q4.1: Explain your feature engineering strategy. Write down atleast 5 new features (apart from the unigram and bigram features) that you have newly implemented for this section. It is okay if those features did not improve performance, still write them here and include your intuition <font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a46303e",
   "metadata": {
    "id": "4a46303e"
   },
   "source": [
    "## Submission\n",
    "You will make two separate submisisons for this hw assignment on gradescope.\n",
    "\n",
    "\n",
    "### hw1-programming\n",
    "You will submit the 2 python files: `models.py` and `features.py` to this assignment on gradescope.\n",
    "\n",
    "Note that we will grade:\n",
    "1. The correctness of your individual function implementations.\n",
    "2. The performance of your trained ``LogisticRegressionClassifier`` model using the ``UnigramFeatureExtractor`` and the ``BigramFeatureExtractor`` on the validation set. These values should ideally be greater than 64% with the following hyperparameters: max_num_features=2000, learning_rate=0.01, epochs=10. Lower values will not get a 0 but be graded based on difference with this reference value.\n",
    "3. The performance of your trained ``LogisticRegressionClassifier`` model using the ``FancyFeatureExtractor`` on the validation set. These values should ideally be greater than 68% with the following hyperparameters: max_num_features=2000, learning_rate=0.01, epochs=10. Lower values will not get a 0 but be graded based on difference with this reference value.\n",
    "4. We will also run your ``LogisticRegressionClassifier`` with your ``FancyFeatureExtractor`` on our hidden test set. You will be graded based on your models' performance on this hidden test set.\n",
    "\n",
    "### hw1-written\n",
    "You will submit the answers to the written questions in this notebook as part of the PDF which contains your responses to ``Part A: Conceptual Questions`` part of the hw1."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tmux-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
